# section 12: amazon s3 intro
* basics 
	* store objects (files) in buckets
		* bucket name: *globally* unique
		* Bs exist at *region* level
	* objects
		* key: full path to obj at bucket level. e.g. s3:/bucket/***this-is/the-key.txt***
			* composed of prefix ('folder' names) and object name
		* "value": content of obj body
			* max size: 5TB
			* if >5GB, must use multipart upload. Rec'd for >100mb, for speed
	* versioning
		* enabled @ bucket level
		* "null" v. for any file made prior to enabling, starts tracking Vs when enabled
		* old Vs not lost if versioning disabled, just no new ones created
		* deleting obj adds delete market, does not remove V history
			* permanent delete: deleting DM or V
* s3 object encryption ^b121a7
	* can specify at obj or bucket level (if bucket, all new obj will use it)
	* 4 encryption methods...
		* SSE-S3: keys fully managed by AWS
		* SSE-KMS: AWS key management service. More user control
		* SSE-C: you manage own keys
		* Client side encryption: all you, s3 has no knowledge
	* SSE-S3
		* AES-256 encryption type
		* to set: set header "x=amx-server-side-encryption":"AES256"
		* completely owned/managed by AWS
	* SSE-KMS
		* keys handled/managed by KMS but you control policy (e.g. key rotation policy)
		* to use: header as above, val = "AWS:KMS"
	* SSE-C:
		* SSE using keys fully managed by customer, outside of AWS
			* S3 does not see the encryption key
		* *must* use HTTPS protocol since secret being sent (all others can use either)
		* key must be provided in headers for each request made
			* much more work for you, need to track all keys used for what. Can't use AWS console to upload
	* Client side encryption
		* libraries can help-- e.g. Amazon s3 encryption client
		* client (you) responsible for encryption & decryption, outside of s3
	~
	* encryption in transit
		* HTTP endpoints: not encrypted
		* HTTPS: "encryption in flight" with SSL & TLS certs
* s3 security
	* two types... 
		* user based
			* IAM: policies attached to IAM users specify how/with what they can interact (i.e. what API calls are allowed)
		* resource based
			* S3 Bucket Policies: what an actor can/cannot do on bucket resources
				* allows cross account access
			* Object ACL (access control list)- specify rules at the object level (not super common)
			* Bucket ACL (even less common)
		* >>IAM user can access a resource if allowed by *either* IAM or bucket policy, but an explicit deny from either trumps all
	* s3 bucket policies
		* json based; info includes:
			* Effect: allow or deny
			* Principal: user/account policy applies to
			* Action: API calls being allowed or denied, e.g. "S3:GetObject"
			* Resource: Buckets/objects the policy applies for
		* BP uses include granting pub or cross-account access, forcing obj to be encrypted at upload
	* blocking public access, 4 ways
		* "NEW" or "ANY" ACLs
		* "NEW" or "ANY" public bucket or access point policies
			* "any" used to block x-account access
		* >>for exam, just know there *is* a way to block pub access, it can be set at acct. level, was created to prevent company data leaks
	* Other security points:
		* can access s3 privately through **vpc endpoints**
		* logging & audit
			* s3 access logs, stored in separate s3 bucket
			* API calls can be logged in CloudTrail
		* user security
			* MFA delete: reqs MFA for permanent deletes
			* pre-signed urls grate limited time access for given user
* s3 static websites: simply fill bucket with site contents -> enable static website -> ensure public access for GET reqs
* CORS (cross origin resource sharing)
	* origin:
		* scheme: protocol
		* host: domain
		* port (implied ports are 443 for https, 80 for http)
	* how it works: origin (us) wants to interact with cross-origin (other site)
		1. preflight request, O->CO: "can I interact w you?"
		```html
			OPTIONS/
			Host: www.[other-origin].com
			Origin: https://[our-origin].com
		```

		2. preflight response, CO->O: "yes, here's how"
		```html
			Access-Control-Allow-Origin: [our origin]
			Access-Control-Allow-Methods: [GET/PUT/DELETE etc]
		```
		3. actual request, O->CO, if allowed by preflight res
	* s3 CORS: when to enable? You have 2 buckets (b1 and b2), 1 is static web and 2 holds resources for it. So b1 should have get reqs enabled for `*`, b2 cors only allows gets from b1 origin.
- s3 consistency model: all objects "strongly consistent"
	- read after write consistency (now) guaranteed, w/ no performance impact


# section 14: Advanced S3 and Athena
- s3 MFA-delete
	- if enabled, need MFA code to perform important s3 operations (e.g. perm del, turn off versioning)
	- need versioning to use
	- only bucket owner (i.e. root user) can enable, and can't be enabled through UI console
- s3 access logs: log all calls to S3 bucket (to separate bucket, else loops)
	- logs all reqs, all statuses 
	- use w data analysis tools, or Athena
	- when logging enabled, ACL automatically updated to incl write access to receiving bucket
- s3 replication
	- CRR and SRR: cross-/same- region replication. 
		- Is async. NOT retroactive.
		- reqs versioning, IAM permissions for write access
	- buckets can be across accounts
	- delete operations (delete markers) are **not** replicated by default. Need to specifically enable
	- no replication chaining. If 1->2->3, 1 is not replicated to 3, only objs originating in 2
- s3 presigned urls
	- to generate for...
		- downloads: CLI or SDK
		- uploads: only SDK
	- default timeout: 3600 secs, can change
	- users inherit permissions of whoever created the URL for get/put reqs
- s3 storage classes
	- types...
		- s3 standard general purpose
		- s3 standard infrequent access
		- s3 one-zone IA
		- s3 intelligent tiering
		- Glacier
		- Glacier Deep Archive
	- s3 standard GP
		- high durability (99.999999999% - 11 9s) and availability (99.99%)
		- 3 AZs, so can withstand 2 zone failures
	- s3 standard IA
		- rapid access but less frequent
		- same retrieval price as standard Glacier ($.01/GB)
		- same durability, facility failure as GP, 99.9% availability
	- s3 one-zone IA
		- like IA but data only in one AZ
		- 99.5% avail, same durability except zone failure -> complete loss
		- 20% cheaper than standard IA
	- s3 intelligent tiering
		- auto moves btw standard GP & IA based on access patterns (not glacier)
		- small additional monitoring and auto-tiering fees
	- glacier
		- cheap. $.004/GB/month + retrieval cost
		- archives := objects, vaults := buckets
		- objects can be <= 40TB(!)
		- flavors...
			-  glacier (regular): min storage duration- 90 days
				-  expedited access: 1-5mins
				-  standard: 3-5hrs
				-  bulk: 5-12hrs
			-  glacier deep archive: min storage- 180 days
				-  standard: 12hr
				-  bulk: 48hr
-  s3 lifecycle rules
	-  can be authorized w a *lifecycle configuration*
	-  lifecycle rules incl...
		-  transition actions: when to move obj to new storage class
		-  expiration actions: set obj to delete after some amt of time
			-  incl. access logs, old versions, incomplete multi-part uploads
	-  can create rules based on prefixes, tags
	-  how do you know what it makes sense (cost-wise) to transition? 
		-  s3 Analytics! report updated daily
			-  helps determine when to go from s3 standard -> IA (not glacier)
-  baseline performance
	-  per prefix: 3500 PUT/COPY/POST/DELETE, 5500 GET/HEAD reqs/second
		-  no limit to number of prefixes
	-  latency: very low, 100-200ms
	-  KMS limitations (with sse-kms)
		-  region-dependent quota/sec limits on upload/download API calls (range from 5500 to 30k). Can't request increase.
	-  s3 Transfer Acceleration: inc. speed by sending data to edge location first (via. public int) which will forward to region over AWS network
	-  s3 Byte Range Fetches: parallelize GETs by req'ing specific byte ranges of file. Each of these req's can be done concurrently.
-  s3 Select and Glacier Select
	-  use server side filtering (SQL, only simple column/row filtering)
	-  processing is server side -> less network transfer -> cheaper, faster (<= 400%!)
-  s3 event notifications (e.g. s3:ObjectRemoved)
	-  can create rules, e.g. certain actions or by file name
	-  three possible targets...	
		-  SNS: simple notification service (notifications, emails)
		-  SQS: simple queue service (add messages to queue) - ensure access policy is set to allow write
		-  Lambda: custom code
	-  typically delivered in seconds
	-  enable versioning to make sure every event is caught (else concurrent operations could be missed)
-  s3 requester pays
	-  when enabled, requester pays for networking. Bucket owner still pays storage
		-  requester must be identified w/in s3, for billing
-  AWS Athena
	-  serverless service for analytics directly against S3 files
		-  usually you'd retrieve, load into db, then run queries
	-  uses SQL, has JDBC/ODBC driver, supports many file types
	-  charged/query & amt. of data scanned
-  Glacier vault lock
	-  to adopt WORM model (write once read many)
	-  can lock policy so can't be changed
	-  why? Data retention, compliance-- guarantees from AWS that no one has edited
-  s3 Object Lock (reqs versioning)
	-  WORM model, blocks version deletion for some specified pd. of time
	-  two options for retention modes:
		-  governance: special permissions req'd to overwrite/delete obj version or change lock settings
		-  compliance: no one (incl. root) can overwrite/delete obj v., and once set, retention mode can't change and pd. can't shorten
	-  two options for obj retention length...
		-  retention period: specify some fixed pd. for lock
		-  legal hold: same protection, no expiry


# section 16: AWS storage extras
- AWS Snow Family
	- collection of secure, portable devices used for two things: data migration or edge computing
		- data mig: for when network uploads are too slow (recommended if network would take >1wk)
		- edge computing: run EC2 instances and Lambda funcs (using AWS IoT Greengrass)
	- device options:
		- snowcone: 8TB, 2CPUs, 4GB mem, wired or wireless
			- use for <=24TB
			- return to AWS offline or w/ AWS DataSync (over int., e.g. if you want to take to remote/harsh location for data collection)
		- snowball (edge): 
			- use <= 10PB, block and s3 storage compatible. Two flavors...
				- storage optimized: 80TB HDD, 40 vCPUs, 80GiB ram
				- compute optimized: 42TB HDD, 52 vCPUs, 208GiB ram, optional GPU
			- snowball can't send data directly into glacier, instead: snowball -> s3 -> //lifecycle policy// -> glacier
		- snowmobile: 100PB -- only data migration! (literal truck)
	- aws opshub: software for managing snow devices
- aws storage gateway
	- hybrid cloud/on-prem storage service
	- 3 flavors...
		- File Gateway
		- volume gateway
		- tape gateway
	- file gateway
		- access s3 buckets w NFS & SMB protocol
		- use with iam roles, can integrate with active directory (AD) for user authentication
		- caches most recently used data
		- can be mounted on many on-prem servers
	- volume gateway
		- block storage (iSCSI protocol) backed by s3
		- backed up w EBS snapshots (for point-in-time copies)
		- two flavors...
			- cached: data primarily stored in S3, cached locally
			- stored: primary storage is local, async backups to s3
	- tape gateway
		- for companies doing backups with physical tapes
		- uses a virtual tape library (VTL), backed by s3 and glacier
		- data backed up using existing tape-based processes & iSCSI interface
	- the gateway has to be installed/run in your data center
		- if you don't have servers to run it, you can use Storage Gateway Hardware Appliance (from aws, install in infra, set up as any of the GW types)
- Amazon FSx (file server for Windows)
	- bc EFS is for linux systems!
	- accessible from on-prem infra, configurable for multi AZs
	- daily backups to s3
	- *not* elastic: provision in advance for storage and throughput capacity
- Amazon FSx for lustre
	- lustre: "linux" + "cluster": parallel distributed FS for large scale, high performance computing
	- seamless integration with s3, can:
		- read s3 as a file system
		- write computation output back to s3
	- can be used from on-prem servers